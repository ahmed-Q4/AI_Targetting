<<<<<<< HEAD
=======
<<<<<<< HEAD
model_tt1 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification)
model_tt2 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification, weights.on.x = "robCov")
# 3.2 Bias reduction in binomial-response generalized linear models: brglm package
# penalized likelihood where penalization is by Jeffreys invariant prior.
m_Logit_BiasRedution <- brglm::brglm(Buy_Sell ~ ., family = binomial(), data = Training_data_classification)
# 3.3 Baysian GLM
m_Logit_Bayesian <- arm::bayesglm(Buy_Sell ~ ., family=binomial(link="probit"), data = Training_data_classification)
# 4) Trying smaller model by utilizing the bestglm package -----
# https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html
# http://www2.uaem.mx/r-mirror/web/packages/bestglm/vignettes/bestglm.pdf
Xy <- cbind(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")],
Bell_Sell = Training_data_classification[,"Buy_Sell"])
# Produced error: Error in rep(-Inf, 2^p) : invalid 'times' argument
best_glm_binomial_probit <- bestglm::bestglm(Xy, family=binomial(link="probit"), method = "forward")
# http://stackoverflow.com/questions/12012746/bestglm-alternatives-for-dataset-with-many-variables
rm(Xy)
# 5) Smaller subset of predictor using the glmnet package ------
# ref:
# http://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet
# https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf
#
Y <- as.numeric(Training_data_classification[,"Buy_Sell"])
X <- as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")])
m_glmnet_lasso <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 1) ## lasso regression - Sparse coeff
m_glmnet_ridge <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 0) ## ridge regression - full coeff
N_folds <- 10
fold_id <- sample(1:N_folds,size=length(Y),replace=TRUE)
m_cv_glmnet_lasso <- cv.glmnet(X,Y, foldid=fold_id, alpha=1, family = "binomial", type.measure="class") ## lasso regression - Sparse coeff
m_cv_glmnet_ridge <- cv.glmnet(X,Y, foldid=fold_id, alpha=0, family = "binomial", type.measure="class") ## ridge
install.packages("ROCR")
library(glmnet)
install.packages("bestglm")
m_Linear <- lm(Position_change ~ ., data = Training_data_regression)
# 2) Logit/Probit Binomial GLM Model -----
m_Logit <- glm(Buy_Sell ~ ., family=binomial(), data = Training_data_classification)
perf_plot(mod = m_Logit, y = Training_data_classification[,"Buy_Sell"])
# When using plain glm, we get a warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
#
# ref:
# http://www.carlislerainey.com/papers/separation.pdf
#
# 3) Trying different proposed model/solution -----
# 3.1 Robust GLM from robustbase package
model_tt1 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification)
model_tt2 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification, weights.on.x = "robCov")
# 3.2 Bias reduction in binomial-response generalized linear models: brglm package
# penalized likelihood where penalization is by Jeffreys invariant prior.
m_Logit_BiasRedution <- brglm::brglm(Buy_Sell ~ ., family = binomial(), data = Training_data_classification)
# 3.3 Baysian GLM
m_Logit_Bayesian <- arm::bayesglm(Buy_Sell ~ ., family=binomial(link="probit"), data = Training_data_classification)
# 4) Trying smaller model by utilizing the bestglm package -----
# https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html
# http://www2.uaem.mx/r-mirror/web/packages/bestglm/vignettes/bestglm.pdf
Xy <- cbind(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")],
Bell_Sell = Training_data_classification[,"Buy_Sell"])
# Produced error: Error in rep(-Inf, 2^p) : invalid 'times' argument
best_glm_binomial_probit <- bestglm::bestglm(Xy, family=binomial(link="probit"), method = "forward")
# http://stackoverflow.com/questions/12012746/bestglm-alternatives-for-dataset-with-many-variables
rm(Xy)
# 5) Smaller subset of predictor using the glmnet package ------
# ref:
# http://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet
# https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf
#
Y <- as.numeric(Training_data_classification[,"Buy_Sell"])
X <- as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")])
m_glmnet_lasso <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 1) ## lasso regression - Sparse coeff
m_glmnet_ridge <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 0) ## ridge regression - full coeff
N_folds <- 10
fold_id <- sample(1:N_folds,size=length(Y),replace=TRUE)
m_cv_glmnet_lasso <- glmnet::cv.glmnet(X,Y, foldid=fold_id, alpha=1, family = "binomial", type.measure="class") ## lasso regression - Sparse coeff
m_cv_glmnet_ridge <- glmnet::cv.glmnet(X,Y, foldid=fold_id, alpha=0, family = "binomial", type.measure="class") #
# 1. Linear Gaussian Model----------
m_Linear <- lm(Position_change ~ ., data = Training_data_regression)
# 2) Logit/Probit Binomial GLM Model -----
m_Logit <- glm(Buy_Sell ~ ., family=binomial(), data = Training_data_classification)
ROCR::perf_plot(mod = m_Logit, y = Training_data_classification[,"Buy_Sell"])
# When using plain glm, we get a warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
#
# ref:
# http://www.carlislerainey.com/papers/separation.pdf
#
# 3) Trying different proposed model/solution -----
# 3.1 Robust GLM from robustbase package
model_tt1 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification)
model_tt2 <- robustbase::glmrob(Buy_Sell ~ ., family=binomial(), data = Training_data_classification, weights.on.x = "robCov")
# 3.2 Bias reduction in binomial-response generalized linear models: brglm package
# penalized likelihood where penalization is by Jeffreys invariant prior.
m_Logit_BiasRedution <- brglm::brglm(Buy_Sell ~ ., family = binomial(), data = Training_data_classification)
# 3.3 Baysian GLM
m_Logit_Bayesian <- arm::bayesglm(Buy_Sell ~ ., family=binomial(link="probit"), data = Training_data_classification)
# 4) Trying smaller model by utilizing the bestglm package -----
# https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html
# http://www2.uaem.mx/r-mirror/web/packages/bestglm/vignettes/bestglm.pdf
Xy <- cbind(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")],
Bell_Sell = Training_data_classification[,"Buy_Sell"])
# Produced error: Error in rep(-Inf, 2^p) : invalid 'times' argument
best_glm_binomial_probit <- bestglm::bestglm(Xy, family=binomial(link="probit"), method = "forward")
# http://stackoverflow.com/questions/12012746/bestglm-alternatives-for-dataset-with-many-variables
rm(Xy)
# 5) Smaller subset of predictor using the glmnet package ------
# ref:
# http://stats.stackexchange.com/questions/77546/how-to-interpret-glmnet
# https://web.stanford.edu/~hastie/Papers/Glmnet_Vignette.pdf
#
Y <- as.numeric(Training_data_classification[,"Buy_Sell"])
X <- as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")])
m_glmnet_lasso <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 1) ## lasso regression - Sparse coeff
m_glmnet_ridge <- glmnet::glmnet(x = X, y = Y , family = "binomial", alpha = 0) ## ridge regression - full coeff
N_folds <- 10
fold_id <- sample(1:N_folds,size=length(Y),replace=TRUE)
m_cv_glmnet_lasso <- glmnet::cv.glmnet(X,Y, foldid=fold_id, alpha=1, family = "binomial", type.measure="class") ## lasso regression - Sparse coeff
m_cv_glmnet_ridge <- glmnet::cv.glmnet(X,Y, foldid=fold_id, alpha=0, family = "binomial", type.measure="class") ## ridge regression - full coeff
library(pROC)
# http://stats.stackexchange.com/questions/133320/logistic-regression-class-probabilities
# http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit
#
# if your classifier were aiming to evaluate a diagnostic test for a serious disease that has a relatively safe cure,
# the sensitivity is far more important that the specificity.
# In another case, if the disease were relatively minor and the treatment were risky, specificity would be more important to control.
# For general classification problems, it is considered "good" to jointly optimize the sensitivity and specification.
#
# The threshold can be determined in a variety of way. Check the OptimalCutpoints package
# http://stats.stackexchange.com/questions/29719/how-to-determine-best-cutoff-point-and-its-confidence-interval-using-roc-curve-i
# Specifying the cost function
cost_fn <- function(analysis, sensitivity_cost = 1, specificity_cost = 1) {
res <- (sensitivity_cost * analysis$sensitivities) + (specificity_cost * analysis$specificities)
return(res)
}
#apply roc function for Logit model
analysis <- roc(response=Training_data_classification$Buy_Sell, predictor=m_Logit$fitted.values)
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_Logit <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for baysian Logit model
analysis <- roc(response=Training_data_classification$Buy_Sell, predictor=m_Logit_Bayesian$fitted.values)
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_bayes <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for glmnet Lasso model
analysis <- roc(response=Training_data_classification$Buy_Sell,
predictor=predict(m_cv_glmnet_lasso,
newx = as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")]),
s = "lambda.min", type = "response"))
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_glmnet_lasso <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for glmnet Ridge model
analysis <- roc(response=Training_data_classification$Buy_Sell,
predictor=predict(m_cv_glmnet_ridge,
newx = as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")]),
s = "lambda.min", type = "response"))
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_glmnet_ridge <- subset(e,e[,2]==max(e[,2]))[,1]
opt_t_models <- c(logit = opt_t_Logit, Baysian = opt_t_bayes,GLMnet_Lasso = opt_t_glmnet_lasso, GLMnet_Ridge = opt_t_glmnet_ridge)
# All model yields similar optimal threshold, and so we will average them and use that estimates to decide on the class when needed
opt_t <- mean(opt_t_models)
# Determining the threshold Method 2 ------
library(ROCR)
# https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/
# Logit model
pred_logit <- prediction(predictions = m_Logit$fitted.values, labels = Training_data_classification$Buy_Sell)
cost.perf <- performance(pred_logit, "cost")
cost.perf_assymetric  <- performance(pred_logit, "cost", cost.fp = 2, cost.fn = 1)
cost.perf_assymetric2 <- performance(pred_logit, "cost", cost.fp = 1, cost.fn = 2)
cost.perf_assymetric3 <- performance(pred_logit, "cost", cost.fp = 1, cost.fn = 5)
# Finding optimal threshold which minimize the cost
# Minimizing cost associated with (False Positive, False Negative)
opt_t2 <- pred_logit@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
opt_t3 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric@y.values[[1]])]
opt_t4 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric2@y.values[[1]])]
opt_t5 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric3@y.values[[1]])]
ModelPreictionResults <- function(threshold) {
# This function access Test_data_regression, Test_data_classification
# as well as regression models defined in the global workspace
Results <- Test_data_regression
Results$Buy_Sell <- as.factor(ifelse(Test_data_regression[,Y_var] > 0, "Buy", "Sell"))
# 1) Linear (Gaussian) Model
Y_pred <- predict(m_Linear,  Test_data_regression[, which(names(Test_data_regression) != Y_var)]) %>% as.vector()
Results$pred_linear_Gaussian_Buy_Sell <- Y_pred
# 2) GLM Logistic (plain) Model
Y_pred <- predict(m_Logit,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
Results$prob_plain_logit_Buy_Sell <- Y_pred
Results$pred_plain_logit_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
# 3) Bias Reduction GLM
Y_pred <- predict(m_Logit_BiasRedution,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
# For some reason, prediction with this model returns class not prob
Results$pred_BiasReduction_logit_Buy_Sell <-  ifelse(abs(Y_pred - 1) == .Machine$double.eps, "Sell", "Buy")
# 4) Bayesian GLM
Y_pred <- predict(m_Logit_Bayesian,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
Results$prob_Bayesian_logit_Buy_Sell <- Y_pred
Results$pred_Bayesian_logit_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
# 5) glmnet model
# To get class, use: type="class"; to get probabilities use: type="response"
# http://stackoverflow.com/questions/26806902/how-to-get-probabilities-between-0-and-1-using-glmnet-logistic-regression
x_test <- as.matrix(Test_data_classification[,which(names(Test_data_classification) != "Buy_Sell")])
# Lasso Results
Y_pred = predict(m_cv_glmnet_lasso, s='lambda.min', newx=x_test, type="response")  %>% as.vector()
Results$prob_glmnet_lasso_Buy_Sell <- Y_pred
Results$pred_glmnet_lasso_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
Y_pred = predict(m_cv_glmnet_lasso, s='lambda.min', newx=x_test, type="class")  %>% as.vector()
Results$pred_glmnet_lasso_Buy_Sell_check <- ifelse(Y_pred == 2, "Sell", "Buy")
# Ridge Results
Y_pred = predict(m_cv_glmnet_ridge, s='lambda.min', newx=x_test, type="response")  %>% as.vector()
Results$prob_glmnet_ridge_Buy_Sell <-  Y_pred
Results$pred_glmnet_ridge_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
Y_pred = predict(m_cv_glmnet_ridge, s='lambda.min', newx=x_test, type="class")  %>% as.vector()
Results$pred_glmnet_ridge_Buy_Sell_check <-  ifelse(Y_pred == 2, "Sell", "Buy")
## NOTE: ------
# It seems that GLMnet uses a probability threshold of 0.5, So we will be using the threshold to determine the class.
# Summary of Model Comparaison based on predictions ----
Results_focus <- Results[,c(1,47:58)]
Results_focus$pred_Buy_given_Buy_LinearGaussian   <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) > 0 & sign(Results[, Y_var]) > 0, TRUE, FALSE)
Results_focus$pred_Buy_given_Sell_LinearGaussian  <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) > 0 & sign(Results[, Y_var]) < 0, TRUE, FALSE)
Results_focus$pred_Sell_given_Buy_LinearGaussian  <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) < 0 & sign(Results[, Y_var]) > 0, TRUE, FALSE)
Results_focus$pred_Sell_given_Sell_LinearGaussian <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) < 0 & sign(Results[, Y_var]) < 0, TRUE, FALSE)
Results_focus$Correct_pred_LinearGaussian         <- ifelse(sign(Results[, Y_var]) == sign(Results$pred_linear_Gaussian_Buy_Sell), TRUE, FALSE)
Results_focus <- Results_focus %>%
dplyr::mutate(# Plain Logistic Regression
pred_Buy_given_Buy_Logistic_plain   = ifelse(pred_plain_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Logistic_plain  = ifelse(pred_plain_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Logistic_plain  = ifelse(pred_plain_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Logistic_plain = ifelse(pred_plain_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_plain         = ifelse(pred_plain_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
# Logistic Bias Reduction
pred_Buy_given_Buy_BiasReduction    = ifelse(pred_BiasReduction_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_BiasReduction   = ifelse(pred_BiasReduction_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_BiasReduction   = ifelse(pred_BiasReduction_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_BiasReduction  = ifelse(pred_BiasReduction_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_BiasReduction = ifelse(pred_BiasReduction_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
#Baysian Logistic
pred_Buy_given_Buy_Bayesian    = ifelse(pred_Bayesian_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Bayesian   = ifelse(pred_Bayesian_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Bayesian   = ifelse(pred_Bayesian_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Bayesian  = ifelse(pred_Bayesian_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_Bayesian = ifelse(pred_Bayesian_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
# GLMnet Lasso
pred_Buy_given_Buy_Lasso    = ifelse(pred_glmnet_lasso_Buy_Sell == "Buy" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == "Buy" & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Lasso  = ifelse(pred_glmnet_lasso_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_GLMnet_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == Buy_Sell, TRUE, FALSE),
# GLMnet Ridge
pred_Buy_given_Buy_Ridge    = ifelse(pred_glmnet_ridge_Buy_Sell == "Buy" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == "Buy" & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Ridge  = ifelse(pred_glmnet_ridge_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_GLMnet_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == Buy_Sell, TRUE, FALSE))
# Summarizing the results and computing Probability Statistics
#browser()
Results_Summary <- Results_focus %>%
dplyr::summarise(Count = as.integer(n()),
Count_Buy  = as.integer(sum(Buy_Sell == "Buy")),
Count_Sell = as.integer(sum(Buy_Sell == "Sell")),
Percent_buy = paste(round(100 * Count_Buy/Count, digits = 2), "%"),
Percent_sell =  paste(round(100 * Count_Sell/Count,digits = 2), "%"),
# LinearGaussian
Prob_Buy_given_Buy_LinearGaussian   = paste(round(100 * sum(pred_Buy_given_Buy_LinearGaussian == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_LinearGaussian  = paste(round(100 * sum(pred_Buy_given_Sell_LinearGaussian == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_LinearGaussian  = paste(round(100 * sum(pred_Sell_given_Buy_LinearGaussian == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_LinearGaussian = paste(round(100 * sum(pred_Sell_given_Sell_LinearGaussian == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_LinearGaussian             = paste(round(100 * sum(Correct_pred_LinearGaussian == TRUE)/Count,digits = 2), "%"),
# Logit Plain
Prob_Buy_given_Buy_Logistic_plain   = paste(round(100 * sum(pred_Buy_given_Buy_Logistic_plain == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Logistic_plain  = paste(round(100 * sum(pred_Buy_given_Sell_Logistic_plain == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Logistic_plain  = paste(round(100 * sum(pred_Sell_given_Buy_Logistic_plain == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Logistic_plain = paste(round(100 * sum(pred_Sell_given_Sell_Logistic_plain == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_Logistic_plain             = paste(round(100 * sum(Correct_pred_Logistic_plain == TRUE)/Count,digits = 2), "%"),
# Logistic Bias Reduction
Prob_Buy_given_Buy_BiasReduction   = paste(round(100 * sum(pred_Buy_given_Buy_BiasReduction == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_BiasReduction  = paste(round(100 * sum(pred_Buy_given_Sell_BiasReduction == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_BiasReduction  = paste(round(100 * sum(pred_Sell_given_Buy_BiasReduction == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Sell_BiasReduction = paste(round(100 * sum(pred_Sell_given_Sell_BiasReduction == TRUE)/Count_Buy,digits = 2),"%"),
accuracy_BiasReduction             = paste(round(100 * sum(Correct_pred_Logistic_BiasReduction == TRUE)/Count,digits = 2), "%"),
# Baysian Logistic
Prob_Buy_given_Buy_Bayesian   = paste(round(100 * sum(pred_Buy_given_Buy_Bayesian == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Bayesian  = paste(round(100 * sum(pred_Buy_given_Sell_Bayesian == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Bayesian  = paste(round(100 * sum(pred_Sell_given_Buy_Bayesian == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Bayesian = paste(round(100 * sum(pred_Sell_given_Sell_Bayesian == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_Bayesian             = paste(round(100 * sum(Correct_pred_Logistic_Bayesian == TRUE)/Count,digits = 2), "%"),
# GLMnet Lasso
Prob_Buy_given_Buy_Lasso   = paste(round(100 * sum(pred_Buy_given_Buy_Lasso == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Lasso  = paste(round(100 * sum(pred_Buy_given_Sell_Lasso == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Lasso  = paste(round(100 * sum(pred_Sell_given_Buy_Lasso == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Lasso = paste(round(100 * sum(pred_Sell_given_Sell_Lasso == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_GLMnet_Lasso      = paste(round(100 * sum(Correct_pred_GLMnet_Lasso == TRUE) /Count,digits = 2),"%"),
#GLMnet Ridge
Prob_Buy_given_Buy_Ridge   = paste(round(100 * sum(pred_Buy_given_Buy_Ridge == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Ridge  = paste(round(100 * sum(pred_Buy_given_Sell_Ridge == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Ridge  = paste(round(100 * sum(pred_Sell_given_Buy_Ridge == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Ridge = paste(round(100 * sum(pred_Sell_given_Sell_Ridge == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_GLMnet_Ridge      = paste(round(100 * sum(Correct_pred_GLMnet_Ridge == TRUE)/Count, digits = 2), "%")) %>%
t()
probability_threshold   = paste0("Threshold: ",round(100 * threshold, digits = 2), "%")
colnames(Results_Summary) <- probability_threshold
res <- list(Summary = Results_Summary, TestResults = Results_focus, Thresh = threshold)
return(res)
}
Results_Summary_1 <- ModelPreictionResults(threshold = opt_t )$Summary # Result summary associated with opt_t
Results_Summary_2 <- ModelPreictionResults(threshold = opt_t2)$Summary # Result summary associated with opt_t2
Results_Summary_3 <- ModelPreictionResults(threshold = opt_t3)$Summary # Result summary associated with opt_t3
Results_Summary_4 <- ModelPreictionResults(threshold = opt_t4)$Summary # Result summary associated with opt_t4
Results_Summary_5 <- ModelPreictionResults(threshold = opt_t5)$Summary # Result summary associated with opt_t5
Results_Summary_all <- cbind(Results_Summary_1, Results_Summary_2, Results_Summary_3, Results_Summary_4, Results_Summary_5)
View(Results_Summary_all)
opt_t
library(pROC)
install.packages("pROC")
library(pROC)
# http://stats.stackexchange.com/questions/133320/logistic-regression-class-probabilities
# http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit
#
# if your classifier were aiming to evaluate a diagnostic test for a serious disease that has a relatively safe cure,
# the sensitivity is far more important that the specificity.
# In another case, if the disease were relatively minor and the treatment were risky, specificity would be more important to control.
# For general classification problems, it is considered "good" to jointly optimize the sensitivity and specification.
#
# The threshold can be determined in a variety of way. Check the OptimalCutpoints package
# http://stats.stackexchange.com/questions/29719/how-to-determine-best-cutoff-point-and-its-confidence-interval-using-roc-curve-i
# Specifying the cost function
cost_fn <- function(analysis, sensitivity_cost = 1, specificity_cost = 1) {
res <- (sensitivity_cost * analysis$sensitivities) + (specificity_cost * analysis$specificities)
return(res)
}
#apply roc function for Logit model
analysis <- roc(response=Training_data_classification$Buy_Sell, predictor=m_Logit$fitted.values)
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_Logit <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for baysian Logit model
analysis <- roc(response=Training_data_classification$Buy_Sell, predictor=m_Logit_Bayesian$fitted.values)
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_bayes <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for glmnet Lasso model
analysis <- roc(response=Training_data_classification$Buy_Sell,
predictor=predict(m_cv_glmnet_lasso,
newx = as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")]),
s = "lambda.min", type = "response"))
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_glmnet_lasso <- subset(e,e[,2]==max(e[,2]))[,1]
#apply roc function for glmnet Ridge model
analysis <- roc(response=Training_data_classification$Buy_Sell,
predictor=predict(m_cv_glmnet_ridge,
newx = as.matrix(Training_data_classification[,which(names(Training_data_classification) != "Buy_Sell")]),
s = "lambda.min", type = "response"))
e <- cbind(analysis$thresholds, cost_fn(analysis))
opt_t_glmnet_ridge <- subset(e,e[,2]==max(e[,2]))[,1]
opt_t_models <- c(logit = opt_t_Logit, Baysian = opt_t_bayes,GLMnet_Lasso = opt_t_glmnet_lasso, GLMnet_Ridge = opt_t_glmnet_ridge)
# All model yields similar optimal threshold, and so we will average them and use that estimates to decide on the class when needed
opt_t <- mean(opt_t_models)
# Determining the threshold Method 2 ------
library(ROCR)
# https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/
# Logit model
pred_logit <- prediction(predictions = m_Logit$fitted.values, labels = Training_data_classification$Buy_Sell)
cost.perf <- performance(pred_logit, "cost")
cost.perf_assymetric  <- performance(pred_logit, "cost", cost.fp = 2, cost.fn = 1)
cost.perf_assymetric2 <- performance(pred_logit, "cost", cost.fp = 1, cost.fn = 2)
cost.perf_assymetric3 <- performance(pred_logit, "cost", cost.fp = 1, cost.fn = 5)
# Finding optimal threshold which minimize the cost
# Minimizing cost associated with (False Positive, False Negative)
opt_t2 <- pred_logit@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
opt_t3 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric@y.values[[1]])]
opt_t4 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric2@y.values[[1]])]
opt_t5 <- pred_logit@cutoffs[[1]][which.min(cost.perf_assymetric3@y.values[[1]])]
class(Training_data_classification$Buy_Sell)
opt_t_models
ModelPreictionResults <- function(threshold) {
# This function access Test_data_regression, Test_data_classification
# as well as regression models defined in the global workspace
Results <- Test_data_regression
Results$Buy_Sell <- as.factor(ifelse(Test_data_regression[,Y_var] > 0, "Buy", "Sell"))
# 1) Linear (Gaussian) Model
Y_pred <- predict(m_Linear,  Test_data_regression[, which(names(Test_data_regression) != Y_var)]) %>% as.vector()
Results$pred_linear_Gaussian_Buy_Sell <- Y_pred
# 2) GLM Logistic (plain) Model
Y_pred <- predict(m_Logit,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
Results$prob_plain_logit_Buy_Sell <- Y_pred
Results$pred_plain_logit_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
# 3) Bias Reduction GLM
Y_pred <- predict(m_Logit_BiasRedution,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
# For some reason, prediction with this model returns class not prob
Results$pred_BiasReduction_logit_Buy_Sell <-  ifelse(abs(Y_pred - 1) == .Machine$double.eps, "Sell", "Buy")
# 4) Bayesian GLM
Y_pred <- predict(m_Logit_Bayesian,  Test_data_classification[, which(names(Test_data_classification) != "Buy_Sell")],
type = "response") %>% as.vector()
Results$prob_Bayesian_logit_Buy_Sell <- Y_pred
Results$pred_Bayesian_logit_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
# 5) glmnet model
# To get class, use: type="class"; to get probabilities use: type="response"
# http://stackoverflow.com/questions/26806902/how-to-get-probabilities-between-0-and-1-using-glmnet-logistic-regression
x_test <- as.matrix(Test_data_classification[,which(names(Test_data_classification) != "Buy_Sell")])
# Lasso Results
Y_pred = predict(m_cv_glmnet_lasso, s='lambda.min', newx=x_test, type="response")  %>% as.vector()
Results$prob_glmnet_lasso_Buy_Sell <- Y_pred
Results$pred_glmnet_lasso_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
Y_pred = predict(m_cv_glmnet_lasso, s='lambda.min', newx=x_test, type="class")  %>% as.vector()
Results$pred_glmnet_lasso_Buy_Sell_check <- ifelse(Y_pred == 2, "Sell", "Buy")
# Ridge Results
Y_pred = predict(m_cv_glmnet_ridge, s='lambda.min', newx=x_test, type="response")  %>% as.vector()
Results$prob_glmnet_ridge_Buy_Sell <-  Y_pred
Results$pred_glmnet_ridge_Buy_Sell <- ifelse(Y_pred > threshold, "Sell", "Buy")
Y_pred = predict(m_cv_glmnet_ridge, s='lambda.min', newx=x_test, type="class")  %>% as.vector()
Results$pred_glmnet_ridge_Buy_Sell_check <-  ifelse(Y_pred == 2, "Sell", "Buy")
## NOTE: ------
# It seems that GLMnet uses a probability threshold of 0.5, So we will be using the threshold to determine the class.
# Summary of Model Comparaison based on predictions ----
Results_focus <- Results[,c(1,47:58)]
Results_focus$pred_Buy_given_Buy_LinearGaussian   <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) > 0 & sign(Results[, Y_var]) > 0, TRUE, FALSE)
Results_focus$pred_Buy_given_Sell_LinearGaussian  <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) > 0 & sign(Results[, Y_var]) < 0, TRUE, FALSE)
Results_focus$pred_Sell_given_Buy_LinearGaussian  <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) < 0 & sign(Results[, Y_var]) > 0, TRUE, FALSE)
Results_focus$pred_Sell_given_Sell_LinearGaussian <- ifelse(sign(Results$pred_linear_Gaussian_Buy_Sell) < 0 & sign(Results[, Y_var]) < 0, TRUE, FALSE)
Results_focus$Correct_pred_LinearGaussian         <- ifelse(sign(Results[, Y_var]) == sign(Results$pred_linear_Gaussian_Buy_Sell), TRUE, FALSE)
Results_focus <- Results_focus %>%
dplyr::mutate(# Plain Logistic Regression
pred_Buy_given_Buy_Logistic_plain   = ifelse(pred_plain_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Logistic_plain  = ifelse(pred_plain_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Logistic_plain  = ifelse(pred_plain_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Logistic_plain = ifelse(pred_plain_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_plain         = ifelse(pred_plain_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
# Logistic Bias Reduction
pred_Buy_given_Buy_BiasReduction    = ifelse(pred_BiasReduction_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_BiasReduction   = ifelse(pred_BiasReduction_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_BiasReduction   = ifelse(pred_BiasReduction_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_BiasReduction  = ifelse(pred_BiasReduction_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_BiasReduction = ifelse(pred_BiasReduction_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
#Baysian Logistic
pred_Buy_given_Buy_Bayesian    = ifelse(pred_Bayesian_logit_Buy_Sell == "Buy"  & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Bayesian   = ifelse(pred_Bayesian_logit_Buy_Sell == "Buy"  & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Bayesian   = ifelse(pred_Bayesian_logit_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Bayesian  = ifelse(pred_Bayesian_logit_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_Logistic_Bayesian = ifelse(pred_Bayesian_logit_Buy_Sell == Buy_Sell, TRUE, FALSE),
# GLMnet Lasso
pred_Buy_given_Buy_Lasso    = ifelse(pred_glmnet_lasso_Buy_Sell == "Buy" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == "Buy" & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Lasso  = ifelse(pred_glmnet_lasso_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_GLMnet_Lasso   = ifelse(pred_glmnet_lasso_Buy_Sell == Buy_Sell, TRUE, FALSE),
# GLMnet Ridge
pred_Buy_given_Buy_Ridge    = ifelse(pred_glmnet_ridge_Buy_Sell == "Buy" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Buy_given_Sell_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == "Buy" & Buy_Sell == "Sell", TRUE, FALSE),
pred_Sell_given_Buy_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == "Sell" & Buy_Sell == "Buy", TRUE, FALSE),
pred_Sell_given_Sell_Ridge  = ifelse(pred_glmnet_ridge_Buy_Sell == "Sell" & Buy_Sell == "Sell", TRUE, FALSE),
Correct_pred_GLMnet_Ridge   = ifelse(pred_glmnet_ridge_Buy_Sell == Buy_Sell, TRUE, FALSE))
# Summarizing the results and computing Probability Statistics
#browser()
Results_Summary <- Results_focus %>%
dplyr::summarise(Count = as.integer(n()),
Count_Buy  = as.integer(sum(Buy_Sell == "Buy")),
Count_Sell = as.integer(sum(Buy_Sell == "Sell")),
Percent_buy = paste(round(100 * Count_Buy/Count, digits = 2), "%"),
Percent_sell =  paste(round(100 * Count_Sell/Count,digits = 2), "%"),
# LinearGaussian
Prob_Buy_given_Buy_LinearGaussian   = paste(round(100 * sum(pred_Buy_given_Buy_LinearGaussian == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_LinearGaussian  = paste(round(100 * sum(pred_Buy_given_Sell_LinearGaussian == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_LinearGaussian  = paste(round(100 * sum(pred_Sell_given_Buy_LinearGaussian == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_LinearGaussian = paste(round(100 * sum(pred_Sell_given_Sell_LinearGaussian == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_LinearGaussian             = paste(round(100 * sum(Correct_pred_LinearGaussian == TRUE)/Count,digits = 2), "%"),
# Logit Plain
Prob_Buy_given_Buy_Logistic_plain   = paste(round(100 * sum(pred_Buy_given_Buy_Logistic_plain == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Logistic_plain  = paste(round(100 * sum(pred_Buy_given_Sell_Logistic_plain == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Logistic_plain  = paste(round(100 * sum(pred_Sell_given_Buy_Logistic_plain == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Logistic_plain = paste(round(100 * sum(pred_Sell_given_Sell_Logistic_plain == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_Logistic_plain             = paste(round(100 * sum(Correct_pred_Logistic_plain == TRUE)/Count,digits = 2), "%"),
# Logistic Bias Reduction
Prob_Buy_given_Buy_BiasReduction   = paste(round(100 * sum(pred_Buy_given_Buy_BiasReduction == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_BiasReduction  = paste(round(100 * sum(pred_Buy_given_Sell_BiasReduction == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_BiasReduction  = paste(round(100 * sum(pred_Sell_given_Buy_BiasReduction == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Sell_BiasReduction = paste(round(100 * sum(pred_Sell_given_Sell_BiasReduction == TRUE)/Count_Buy,digits = 2),"%"),
accuracy_BiasReduction             = paste(round(100 * sum(Correct_pred_Logistic_BiasReduction == TRUE)/Count,digits = 2), "%"),
# Baysian Logistic
Prob_Buy_given_Buy_Bayesian   = paste(round(100 * sum(pred_Buy_given_Buy_Bayesian == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Bayesian  = paste(round(100 * sum(pred_Buy_given_Sell_Bayesian == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Bayesian  = paste(round(100 * sum(pred_Sell_given_Buy_Bayesian == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Bayesian = paste(round(100 * sum(pred_Sell_given_Sell_Bayesian == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_Bayesian             = paste(round(100 * sum(Correct_pred_Logistic_Bayesian == TRUE)/Count,digits = 2), "%"),
# GLMnet Lasso
Prob_Buy_given_Buy_Lasso   = paste(round(100 * sum(pred_Buy_given_Buy_Lasso == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Lasso  = paste(round(100 * sum(pred_Buy_given_Sell_Lasso == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Lasso  = paste(round(100 * sum(pred_Sell_given_Buy_Lasso == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Lasso = paste(round(100 * sum(pred_Sell_given_Sell_Lasso == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_GLMnet_Lasso      = paste(round(100 * sum(Correct_pred_GLMnet_Lasso == TRUE) /Count,digits = 2),"%"),
#GLMnet Ridge
Prob_Buy_given_Buy_Ridge   = paste(round(100 * sum(pred_Buy_given_Buy_Ridge == TRUE)  /Count_Buy,digits = 2),"%"),
Prob_Buy_given_Sell_Ridge  = paste(round(100 * sum(pred_Buy_given_Sell_Ridge == TRUE) /Count_Sell,digits = 2),"%"),
Prob_Sell_given_Buy_Ridge  = paste(round(100 * sum(pred_Sell_given_Buy_Ridge == TRUE) /Count_Buy,digits = 2),"%"),
Prob_Sell_given_Sell_Ridge = paste(round(100 * sum(pred_Sell_given_Sell_Ridge == TRUE)/Count_Sell,digits = 2),"%"),
accuracy_GLMnet_Ridge      = paste(round(100 * sum(Correct_pred_GLMnet_Ridge == TRUE)/Count, digits = 2), "%")) %>%
t()
probability_threshold   = paste0("Threshold: ",round(100 * threshold, digits = 2), "%")
colnames(Results_Summary) <- probability_threshold
res <- list(Summary = Results_Summary, TestResults = Results_focus, Thresh = threshold)
return(res)
}
Results_Summary_1 <- ModelPreictionResults(threshold = opt_t )$Summary # Result summary associated with opt_t
Results_Summary_2 <- ModelPreictionResults(threshold = opt_t2)$Summary # Result summary associated with opt_t2
Results_Summary_3 <- ModelPreictionResults(threshold = opt_t3)$Summary # Result summary associated with opt_t3
Results_Summary_4 <- ModelPreictionResults(threshold = opt_t4)$Summary # Result summary associated with opt_t4
Results_Summary_5 <- ModelPreictionResults(threshold = opt_t5)$Summary # Result summary associated with opt_t5
Results_Summary_all <- cbind(Results_Summary_1, Results_Summary_2, Results_Summary_3, Results_Summary_4, Results_Summary_5)
View(Results_Summary_all)
(opt_t, opt_t2, opt_t3, opt_t4, opt_t5)
(opt_t, opt_t2, opt_t3, opt_t4, opt_t5)
c(opt_t, opt_t2, opt_t3, opt_t4, opt_t5)
rank(c(opt_t, opt_t2, opt_t3, opt_t4, opt_t5))
Results_Summary_all <- Results_Summary_all[, rank(c(opt_t, opt_t2, opt_t3, opt_t4, opt_t5))]
View(Results_Summary_all)
Results_Summary_all <- cbind(Results_Summary_5, Results_Summary_4, Results_Summary_2, Results_Summary_1, Results_Summary_3)
View(Results_Summary_all)
opt_t_models
write.csv(Results_Summary_all, "./output_files/Results_Summary.csv", row.names = TRUE, col.names = TRUE)
=======
SharkPositions_good <- data.set <- dplyr::semi_join(SharkPositions, good_symbols_list, by = "Symbol")
data_path <- "~/Data/Fundamentals_9_17_2016/fundamentals_9_17_2016"
files <- dir(path = data_path, pattern = "_Request")
data.set <- dbReadTable(con, "SharkPositions_good") %>% dplyr::mutate(Date = as.Date(Date))
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list[1:10]) {
# Verbose msg
counter <<- counter + 1
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
gc()
}
Positions_dates <- sort(unique(SharkPositions$Date))
data.set <- dbReadTable(con, "SharkPositions_good") %>% dplyr::mutate(Date = as.Date(Date))
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list[1:10]) {
# Verbose msg
counter <<- counter + 1
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
gc()
}
gc()
pryr::mem_used()
save.image("WorkSpace_1tmp.RData")
pryr::mem_used()
for(i in 1:10){
gc()
sleep(0.5)
}
for(i in 1:10){
gc()
Sys.sleep(0.5)
}
pryr::mem_used()
gc()
pryr::mem_used()
Positions_dates <- sort(unique(SharkPositions$Date))
good_symbols_list <- dplyr::filter(transaction_per_symbol_per_year, Count <= 4) %>% dplyr::distinct()
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(), default.file = .rmysql.settingsfile, group = "local_intel")
SharkPositions_good <- data.set <- dplyr::semi_join(SharkPositions, good_symbols_list, by = "Symbol")
data_path <- "~/Data/Fundamentals_9_17_2016/fundamentals_9_17_2016"
saveRDS(data.set, file = "DataSet.rds")
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list[1:8]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS(DataSet.rds)
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list[1:8]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
pryr::mem_used()
counter
for(t in tbls_list[9:17]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
>>>>>>> 0c484179b03a9984cb4a73d42a5e6d8bb53378ce
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(), default.file = .rmysql.settingsfile, group = "local_intel")
dbListTables(con)
dbWriteTable(con, "SharkPositions", SharkPositions)
Positions_dates <- sort(unique(SharkPositions$Date))
good_symbols_list <- dplyr::filter(transaction_per_symbol_per_year, Count <= 4) %>% dplyr::distinct()
SharkPositions_good <- data.set <- dplyr::semi_join(SharkPositions, good_symbols_list, by = "Symbol")
data_path <- "~/Data/Fundamentals_9_17_2016/fundamentals_9_17_2016"
files <- dir(path = data_path, pattern = "_Request")
pryr::mem_used()
counter <- 0
tbls_list <- dbListTables(con)
saveRDS(data.set, file = "DataSet.rds")
for(t in tbls_list[1:9]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
Positions_dates <- sort(unique(SharkPositions$Date))
good_symbols_list <- dplyr::filter(transaction_per_symbol_per_year, Count <= 4) %>% dplyr::distinct()
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(), default.file = .rmysql.settingsfile, group = "local_intel")
SharkPositions_good <- data.set <- dplyr::semi_join(SharkPositions, good_symbols_list, by = "Symbol")
tbls_list <- dbListTables(con)
for(t in tbls_list[10:19]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
counter <- 9
pryr::mem_used()
for(t in tbls_list[10:19]) {
# Verbose msg
counter <<- counter + 1
data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>% dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))]))
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
saveRDS(data.set, file = "DataSet.rds")
rm(data.X)
rm(data.set)
gc()
}
pryr::mem_used()
data.set <<- readRDS("DataSet.rds")
pryr::mem_used()
View(data.set)
NROW(SharkPositions)
NROW(data.set)
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(),  default.file = "~/.my.cnf", group = "local_intel")
dbListTables(con)
dbListTables(con)
data.set <- dbReadTable(con, "SharkPositions_good") %>% dplyr::mutate(Date = as.Date(Date))
# saveRDS(data.set, file = "DataSet.rds")
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list) {
# Verbose msg
counter <<- counter + 1
#data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good", "SharkPositions" , "SharkPositions_good" , "data_set")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>%
dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))])) %>%
dplyr::ungroup() %>%
# Keeping only 1 data point per symbol per Position Date
dplyr::arrange(Symbol, desc(Date)) %>%
dplyr::distinct(Symbol, Next_Pos_date, .keep_all = TRUE)
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
#saveRDS(data.set, file = "DataSet.rds")
names(data.set)
rm(data.X)
gc()
}
data.set <- data.set %>% dplyr::arrange(Symbol, Date)
Positions_dates <- sort(unique(SharkPositions$Date))
data.set <- dbReadTable(con, "SharkPositions_good") %>% dplyr::mutate(Date = as.Date(Date))
# saveRDS(data.set, file = "DataSet.rds")
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
for(t in tbls_list) {
# Verbose msg
counter <<- counter + 1
#data.set <<- readRDS("DataSet.rds")
message(paste0("Processing table number: ", counter, ". TableName/Fundamental data: ", t, ". Memory used: ", pryr::mem_used()))
# Looping through all the tables in the db excluding the sharkpositions table
if (t %in% c("sharkpositions", "sharkpositions_good", "SharkPositions" , "SharkPositions_good" , "data_set")) next
data.X <- dbReadTable(con, t) %>% dplyr::mutate(Date = as.Date(Date)) %>%
dplyr::rowwise() %>%
dplyr::mutate(Next_Pos_date = suppressWarnings(Positions_dates[min(which(Positions_dates > Date))])) %>%
dplyr::ungroup() %>%
# Keeping only 1 data point per symbol per Position Date
dplyr::arrange(Symbol, desc(Date)) %>%
dplyr::distinct(Symbol, Next_Pos_date, .keep_all = TRUE)
# Renaming the variable before adding them to the data_set
names(data.X)[which(names(data.X) == "Date")] <- paste0("Date_", t)
if(NCOL(data.X) > 3) names(data.X)[which(names(data.X) == "Value")] <- t
# Constructing the dataset
data.set <<- dplyr::left_join(data.set, data.X, by = c("Symbol", "Date" = "Next_Pos_date"))
#saveRDS(data.set, file = "DataSet.rds")
names(data.set)
rm(data.X)
gc()
}
data.set <- data.set %>% dplyr::arrange(Symbol, Date)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), # -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.)))
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), # -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.))) %>%
try(dplyr::select(.,-row_names.x, -row_names.y))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), # -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.))) %>%
try(dplyr::select(.,-row_names.x, -row_names.y, -row_names.z))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), # -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.))) %>%
try(dplyr::select(.,-row_names.x, -row_names.y))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), # -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.))) %>%
try(dplyr::select(-row_names.x, -row_names.y))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), -row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.)))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), #-row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.)),
-grep(pattern = "row_names.", x = names(.)))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), #-row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.)),
-grep(pattern = "row_names..", x = names(.)))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)), #-row_names.x, -row_names.y,
-grep(pattern = "lag_", x = names(.)),
-grep(pattern = "roww_names.", x = names(.)))
names(data.set2)
data.set2 <- data.set %>% dplyr::mutate(Position_normalized = Position/NumberHolders,
Position_percent = Position_normalized/SharesOutstanding) %>%
dplyr::group_by(Symbol) %>% dplyr::mutate(lag_Position_normalized = lag(Position_normalized),
Position_normalized_change = Position_normalized - lag_Position_normalized,
lag_Position_percent = lag(Position_percent),
Position_percent_change = Position_percent - lag_Position_percent,
lag_Position = lag(Position),
Position_change =  Position - lag_Position) %>%
dplyr::ungroup() %>%
dplyr::select(-grep(pattern = "Date_", x = names(.)),
-grep(pattern = "lag_", x = names(.)),
-grep(pattern = "row_names.", x = names(.)))
dbWriteTable(con, "data_set", data.set2)
dbWriteTable(con, "data_set", data.set2, overwrite = TRUE)
data.set2 <- dbReadTable(con, "data_set")
data.set3 <- na.omit(data.set2)
Test_years <- c(2015)
Training_years <- c(2008:2014)
regression_data <- data.set3[,c(59,1,3,9:54)] %>%
dplyr::filter(Year %in% Training_years) %>%
dplyr::select(-Symbol, -Date, -Year)
save.image("~/R_workspaces/AI_Targetting/Workspace_01.RData")
library(HighDimOut)
getDoParWorkers()
foreach::getDoParWorkers()
foreach::getDoParWorkers()
load("~/R_workspaces/AI_Targetting/Workspace_02_RegressionReady.RData")
context_var <- c(#"Symbol", "Year",  "Date",
"HasOptions", "SharkGrouping", "NumberHolders", "SharesOutstanding", "FSPermSecId")
Y_var_potential <- grep(pattern = "Position", x = names(data.set3), value = TRUE)
Y_var <- "Position_change"
X_var <-  setdiff(names(data.set3),
union(Y_var_potential, context_var))
# Identification of Training & Testing data set ------
# Random Sample vs Specific years for training set
RANDOM_TRAINING <- FALSE
FRACTION_TRAINING <- 0.75
if(RANDOM_TRAINING == TRUE) {
n_samples <- floor(NROW(data.set3) * FRACTION_TRAINING)
sample_ids <- sample.int(n = NROW(data.set3), size = n_samples, replace = FALSE)
Training_data_regression <- data.set3[sample_ids , c(Y_var, X_var)]
Test_data_regression     <- data.set3[-sample_ids, c(Y_var, X_var)]
} else {
# Splitting training/Testing
Years_dataset <- as.numeric(sort(unique(data.set3$Year)))
Test_years <- c(2015)
Training_years <- setdiff(Years_dataset, Test_years)
Training_data_regression <- dplyr::filter(data.set3, Year %in% Training_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
Test_data_regression <- dplyr::filter(data.set3, Year %in% Test_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
}
library(HighDimOut)
my_func <- function(x) {
print(unique(x$Year))
browser()
data_tmp <- x[, names(x) %in% c("Date", "Year", "Symbol")] %>% as.data.frame()
scaled_data <- scale(x = data_tmp, center = TRUE, scale = TRUE) %>% as.data.frame()
res.ABOD <- Func.ABOD(data=scaled_data, basic=FALSE, perc=0.2)
score.trans.ABOD <- Func.trans(raw.score = score.ABOD, method = "ABOD")
x$ABOD_Score <- score.trans.ABOD
return(x)
}
scanned_data <- data.set3[, c(Y_var, X_var)] %>% # partition() %>%
dplyr::group_by(Year) %>% dplyr::do(res = my_func(.))
names(data_tmp)
my_func <- function(x) {
print(unique(x$Year))
browser()
data_tmp <- x[, !(names(x) %in% c("Date", "Year", "Symbol"))] %>% as.data.frame()
scaled_data <- scale(x = data_tmp, center = TRUE, scale = TRUE) %>% as.data.frame()
res.ABOD <- Func.ABOD(data=scaled_data, basic=FALSE, perc=0.2)
score.trans.ABOD <- Func.trans(raw.score = score.ABOD, method = "ABOD")
x$ABOD_Score <- score.trans.ABOD
return(x)
}
scanned_data <- data.set3[, c(Y_var, X_var)] %>% # partition() %>%
dplyr::group_by(Year) %>% dplyr::do(res = my_func(.))
my_func <- function(x) {
print(unique(x$Year))
data_tmp <- x[, !(names(x) %in% c("Date", "Year", "Symbol"))] %>% as.data.frame()
scaled_data <- scale(x = data_tmp, center = TRUE, scale = TRUE) %>% as.data.frame()
res.ABOD <- Func.ABOD(data=scaled_data, basic=FALSE, perc=0.2)
score.trans.ABOD <- Func.trans(raw.score = score.ABOD, method = "ABOD")
x$ABOD_Score <- score.trans.ABOD
return(x)
}
scanned_data <- data.set3[, c(Y_var, X_var)] %>% # partition() %>%
dplyr::group_by(Year) %>% dplyr::do(res = my_func(.))
foreach::getDoParWorkers()
doParallel::registerDoParallel(cores=detectCores() - 1L)
doParallel::registerDoParallel(cores=Parallel::detectCores() - 1L)
doParallel::registerDoParallel(cores=parallel::detectCores() - 1L)
foreach::getDoParWorkers()
my_func <- function(x) {
print(unique(x$Year))
data_tmp <- x[, !(names(x) %in% c("Date", "Year", "Symbol"))] %>% as.data.frame()
scaled_data <- scale(x = data_tmp, center = TRUE, scale = TRUE) %>% as.data.frame()
res.ABOD <- Func.ABOD(data=scaled_data, basic=FALSE, perc=0.2)
score.trans.ABOD <- Func.trans(raw.score = score.ABOD, method = "ABOD")
x$ABOD_Score <- score.trans.ABOD
return(x)
}
scanned_data <- data.set3[, c(Y_var, X_var)] %>% # partition() %>%
dplyr::group_by(Year) %>% dplyr::do(res = my_func(.))
my_func <- function(x) {
print(unique(x$Year))
data_tmp <- x[, !(names(x) %in% c("Date", "Year", "Symbol"))] %>% as.data.frame()
scaled_data <- scale(x = data_tmp, center = TRUE, scale = TRUE) %>% as.data.frame()
res.ABOD <- Func.ABOD(data=scaled_data, basic=FALSE, perc=0.1)
score.trans.ABOD <- Func.trans(raw.score = score.ABOD, method = "ABOD")
x$ABOD_Score <- score.trans.ABOD
return(x)
}
scanned_data <- data.set3[, c(Y_var, X_var)] %>% # partition() %>%
dplyr::group_by(Year) %>% dplyr::do(res = my_func(.))
<<<<<<< HEAD
RANDOM_TRAINING <- FALSE
FRACTION_TRAINING <- 0.75
if(RANDOM_TRAINING == TRUE) {
n_samples <- floor(NROW(data.set3) * FRACTION_TRAINING)
sample_ids <- sample.int(n = NROW(data.set3), size = n_samples, replace = FALSE)
Training_data_regression <- data.set3[sample_ids , c(Y_var, X_var)]
Test_data_regression     <- data.set3[-sample_ids, c(Y_var, X_var)]
} else {
# Splitting training/Testing
Years_dataset <- as.numeric(sort(unique(data.set3$Year)))
Test_years <- c(2015)
Training_years <- setdiff(Years_dataset, Test_years)
Training_data_regression <- dplyr::filter(data.set3, Year %in% Training_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
Test_data_regression <- dplyr::filter(data.set3, Year %in% Test_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
}
# Classification data sets -----
Training_data_classification <- Training_data_regression
Training_data_classification$Buy_Sell <- as.factor(ifelse(Training_data_regression[,Y_var] > 0, "Buy", "Sell"))
Training_data_classification <- Training_data_classification[, -which(names(Training_data_classification) == Y_var)]
Test_data_classification <- Test_data_regression
Test_data_classification$Buy_Sell <- as.factor(ifelse(Test_data_regression[,Y_var] > 0, "Buy", "Sell"))
Test_data_classification <- Test_data_classification[, -which(names(Test_data_classification) == Y_var)]
library(FSelector)
install.packages("FSelector")
features_information.gain_classification <- information.gain(formula = Buy_Sell ~ ., data = Training_data_classification)
library(FSelector)
library(FSelector)
library(FSelector)
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7') # for 64-bit version
library(rJava)
Sys.setenv(JAVA_HOME='C:\\Program Files (x86)\\Java\\jre1.8.0_111') # for 64-bit version
library(rJava)
library(FSelector)
features_information.gain_classification <- information.gain(formula = Buy_Sell ~ ., data = Training_data_classification)
features_gain.ratio_classification <- gain.ratio(formula = Buy_Sell ~ ., data = Training_data_classification)
features_randomForest <- random.forest.importance(formula = Buy_Sell ~ ., data = Training_data_classification, importance.type = 1)
sapply(Training_data_classification, function(x) class(x))
context_var <- c("Symbol", "Year",  "Date",
"HasOptions", "SharkGrouping", "NumberHolders", "SharesOutstanding", "FSPermSecId")
Y_var_potential <- grep(pattern = "Position", x = names(data.set3), value = TRUE)
Y_var <- "Position_change"
X_var <-  setdiff(names(data.set3),
union(Y_var_potential, context_var))
# Identification of Training & Testing data set ------
# Random Sample vs Specific years for training set
RANDOM_TRAINING <- FALSE
FRACTION_TRAINING <- 0.75
if(RANDOM_TRAINING == TRUE) {
n_samples <- floor(NROW(data.set3) * FRACTION_TRAINING)
sample_ids <- sample.int(n = NROW(data.set3), size = n_samples, replace = FALSE)
Training_data_regression <- data.set3[sample_ids , c(Y_var, X_var)]
Test_data_regression     <- data.set3[-sample_ids, c(Y_var, X_var)]
} else {
# Splitting training/Testing
Years_dataset <- as.numeric(sort(unique(data.set3$Year)))
Test_years <- c(2015)
Training_years <- setdiff(Years_dataset, Test_years)
Training_data_regression <- dplyr::filter(data.set3, Year %in% Training_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
Test_data_regression <- dplyr::filter(data.set3, Year %in% Test_years) %>%
dplyr::select_(.dots = c(Y_var, X_var))
}
# Classification data sets -----
Training_data_classification <- Training_data_regression
Training_data_classification$Buy_Sell <- as.factor(ifelse(Training_data_regression[,Y_var] > 0, "Buy", "Sell"))
Training_data_classification <- Training_data_classification[, -which(names(Training_data_classification) == Y_var)]
Test_data_classification <- Test_data_regression
Test_data_classification$Buy_Sell <- as.factor(ifelse(Test_data_regression[,Y_var] > 0, "Buy", "Sell"))
Test_data_classification <- Test_data_classification[, -which(names(Test_data_classification) == Y_var)]
features_information.gain_classification <- information.gain(formula = Buy_Sell ~ ., data = Training_data_classification)
features_gain.ratio_classification <- gain.ratio(formula = Buy_Sell ~ ., data = Training_data_classification)
features_randomForest <- random.forest.importance(formula = Buy_Sell ~ ., data = Training_data_classification, importance.type = 1)
View(features_randomForest)
View(features_gain.ratio_classification)
View(features_information.gain_classification)
install.packages("rmarkdown")
names(Training_data_classification)
head(data.set3)
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(),  default.file = "~/.my.cnf", group = "local_intel")
dbListTables(con)
data.set <- dbReadTable(con, "SharkPositions_good") %>% dplyr::mutate(Date = as.Date(Date))
tbls_list <- dbListTables(con)
counter <- 0
pryr::mem_used()
library(data.table)
library(bit64)
sec_ticker_exchange <- fread("~/R_workspaces/AI_Targetting/Data from Rob/h_security_ticker_exchange.txt", "|")
write.csv(sec_ticker_exchange, "security_ticker_exchange.csv", row.names = FALSE)
tt <- readRDS("security_ticker_exchange.RDS")
library(data.table)
data.table::fwrite(tt, file = "security_ticker_exchange_with_symbols.csv")
View(tt)
tt <- tt[, ReportingDate := as.Date(REPORT_DATE)]
tt$REPORT_DATE <- as.Date(tt$REPORT_DATE)
class(tt$REPORT_DATE)
head(tt)
library(bit64)
head(tt)
library(RMySQL)
con <- dbConnect(RMySQL::MySQL(),  default.file = "~/.my.cnf", group = "local_intel")
dbWriteTable(con, "Inst_Holdings_hist_w_symbols", tt)
dbWriteTable(con, "Inst_Holdings_hist_w_symbols", as.data.frame(tt))
library(readr)
symbol_SIC_Groups <- read_csv("~/R_workspaces/AI_Targetting/Data from Rob/symbolSICGroups.csv")
names(symbol_SIC_Groups)[-1] <- paste0("SIC_", names(symbol_SIC_Groups)[-1])
names(symbol_SIC_Groups)
tt <- dplyr::left_join(tt, symbol_SIC_Groups, by = "Symbol")
tt2 <- dplyr::filter(tt, is.na(SIC_Group1))
NROW(tt2)
View(tt2)
NROW(tt2)/NROW(tt)
tt3 <- dplyr::filter(tt, grepl(pattern = ".", x = Symbol, fixed = TRUE))
View(tt3)
NROW(tt3)
NROW(tt2)
=======
>>>>>>> 2b3418d1753248fd484b43dfc614f72040f4fb2b
>>>>>>> 0c484179b03a9984cb4a73d42a5e6d8bb53378ce
